% Preamble templated from Mihir-Divyansh/Course-Setup
%iffalse
\let\negmedspace\undefined
\let\negthickspace\undefined
\documentclass[journal,12pt,onecolumn]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{txfonts}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{gensymb}
\usepackage{comment}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide}
\usepackage{listings}
\usepackage{gvv}
%\def\inputGnumericTable{}
\usepackage[latin1]{inputenc}
\usepackage{color}
\usepackage{array}
\usepackage{longtable}
\usepackage{calc}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{ifthen}
\usepackage{lscape}
\usepackage{tabularx}
\usepackage{array}
\usepackage{float}
\usepackage{caption}
\usepackage{multicol}
\usepackage{listings}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{problem}{Problem}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{example}{Example}[section]
\newtheorem{definition}[problem]{Definition}
\newcommand{\BEQA}{\begin{eqnarray}}
\newcommand{\EEQA}{\end{eqnarray}}
%\newcommand{\define}{\stackrel{\triangle}{=}}
\theoremstyle{remark}
%\newtheorem{rem}{Remark}

% Marks the beginning of the document
\begin{document}
\bibliographystyle{IEEEtran}
\vspace{3cm}

\title{Course Project (Software): Image Compression using Truncated SVD}
\author{EE25BTECH11055 - Subhodeep Chakraborty}
\maketitle
\hrulefill
\bigskip

\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}
% intro
% background
% summary
% svd
% how image (trunc)
% algo
% code logic structufe
% ex
% error thetable
% trade offs
\section{INTRODUCTION}
The project's idea is to perform rudimentary compression on an image by representing it as a matrix (or in the case of a colour image, 3 matrices) and applying SVD upon it. By keeping only the top $k$ singular values, a low rank approximation of the image matrix can be obtained, which upon conversion back to the source format results in a smaller file size due to the format's compressive algorithms.
\section{BACKGROUND}
\subsection{Mathematical Intuition}
Professor Gilbert Strang's lecture on the SVD provides an intuition to deal with the problem of working with the SVD. He answers two big questions: What is it, how do we find it.
\par SVD aims to find orthogonal bases for the input and output spaces for any matrix $\vec{A}$, such that when it acts on any vector in the input basis, it produces a scaled vector in the output basis whose scaling factor is called a singular value.
\par For finding the SVD, Prof. Strang uses the square symmetric matrices $\vec{A}^\top\vec{A}$ and $\vec{A}\vec{A}^\top$. By writing SVD equation for $\vec{A}^\top\vec{A}$, we find the equation to result in the eigendecomposition of $\vec{A}^\top\vec{A}$, giving us $\vec{V}$ as its eigenvectors, and the singular values as the square roots of its eigenvalues. Similarly, we can find $\vec{U}$ using $\vec{A}\vec{A}^\top$.
\par Then, Prof. Strang shows why he calls SVD the final and best decomposition: it represents all 4 fundamental subspaces of the matrix. Finally he talks about how $\vec{A}$ can also be represented as a sum instead of a product: it is the sum of the outer products of the left and right simgular vectors, multiplied by the singular value. This representation is especially useful in data science, for image compression, noise reduction, recommendation systems etc.
\subsection{Theory}
The \textbf{Singular Value Decomposition (SVD)} of a matrix is a factorisation of any matrix into the product of an orthogonal, diagonal and orthogonal matrix.
\begin{align}
 \vec{A} = \vec{U}\vec{\Sigma}\vec{V}^\top
\end{align}
Here,
\begin{itemize}
 \item $\vec{U}$ \brak{\textbf{Left Singular Vectors}}: An orthogonal $m \times m$ matrix
 \item $\vec{\Sigma}$ \brak{\textbf{Singular Values}}: A diagonal $m \times n$ matrix. It contains the singular values along its diagonal, in decreasing order. The larger values are more important, i.e., represent more information about the overall matrix.
 \item $\vec{V}$ \brak{\textbf{Right Singular Vectors}}: An orthogonal $n \times n$ matrix
\end{itemize}
This factorisation can also be interpreted as a transformation, which consists of a rotation, followed by a rescaling, and then finally another rotation.
\subsection{Application}
An image is basically a grid of pixels. Thus, a grayscale image can be represented by a matrix $\vec{A} \in \mathbb{R}^{m\times n}$, where each element corresponds to the intensity of a pixel. Similarly for colour images, which can be broken into 3 matrices for Red, Green, and Blue intensities.
A low-rank approximation can be obtained by keeping only the top $k$ singular values.
Thus, we obtain 3 new matrices:
\begin{itemize}
 \item $\vec{U}_k$: The first $k$ columns of $\vec{U}$.
 \item $\vec{\Sigma}_k$: $k$ largest singular values
 \item $\vec{V}^\top_k$: First $k$ rows of $\vec{V}^\top$
\end{itemize}
So the reconstituted matrix is
\begin{align}
 \vec{A}_k = \vec{U}_k\vec{\Sigma}_k\vec{V}^\top_k
\end{align}
Even with very small $k$, good image can be obtained because the top singular values store most of the data.

\section{IMPLEMENTATION}
\subsection{Algorithm}
This project implements an algorithm of the author's own implemenation. The algorithm is inspired from Power Iteration and the Restarted Lanczos Bidiagonalisation. A discussion on the choice of this algorithm versus others and the trade-offs can be found in section \ref{disc}.
\subsubsection{Mathematical Logic}
\begin{align*}
    \text{Given: } & \text{Matrix } A \text{ (size } m \times n \text{), rank } k \\
%     \text{Initialize: } & U_k \text{ (} m \times k \text{), } V_k \text{ (} n \times k \text{), } \Sigma_k \text{ (} k \times k \text{)} \\
    \\
    \text{For } r = 0 \text{ to } k-1: & \\
        & \quad \mathbf{v}^{(0)} \leftarrow \text{random } n \times 1 \text{ vector} \\
        & \quad \mathbf{v}^{(0)} \leftarrow \frac{\mathbf{v}^{(0)}}{\|\mathbf{v}^{(0)}\|_2} \\
        \\
        & \text{(for } i = 0, 1, \dots, \text{max\_iters):} \\
        & \quad \mathbf{v}_{\text{old}} \leftarrow \mathbf{v}^{(i)} \\
        \\
%         & \quad \text{// Left vector step (u = Av)} \\
        & \quad \hat{\mathbf{u}} \leftarrow A \mathbf{v}^{(i)} \\
        & \quad \hat{\mathbf{u}}_{\text{ortho}} \leftarrow \hat{\mathbf{u}} - \sum_{j=0}^{r-1} (\hat{\mathbf{u}}^T \mathbf{u}_j) \mathbf{u}_j &  \\
        & \quad \mathbf{u}^{(i+1)} \leftarrow \frac{\hat{\mathbf{u}}_{\text{ortho}}}{\|\hat{\mathbf{u}}_{\text{ortho}}\|_2} &  \\
        \\
%         & \quad \text{// Right vector step (v = A^T u)} \\
        & \quad \hat{\mathbf{v}} \leftarrow A^T \mathbf{u}^{(i+1)} \\
        & \quad \hat{\mathbf{v}}_{\text{ortho}} \leftarrow \hat{\mathbf{v}} - \sum_{j=0}^{r-1} (\hat{\mathbf{v}}^T \mathbf{v}_j) \mathbf{v}_j &  \\
        & \quad \sigma_r \leftarrow \|\hat{\mathbf{v}}_{\text{ortho}}\|_2 &  \\
        & \quad \mathbf{v}^{(i+1)} \leftarrow \frac{\hat{\mathbf{v}}_{\text{ortho}}}{\sigma_r} & \\
        \\
        & \quad \text{If } (\mathbf{v}^{(i+1)})^T \mathbf{v}_{\text{old}} \approx 1, \text{ break iteration (converged).} \\
        \\
        & \text{Store final components:} \\
        & \quad \mathbf{u}_r \leftarrow \mathbf{u}^{(i+1)} \quad (\text{Store as column } r \text{ of } U_k) \\
        & \quad \mathbf{v}_r \leftarrow \mathbf{v}^{(i+1)} \quad (\text{Store as column } r \text{ of } V_k) \\
        & \quad (\Sigma_k)_{r,r} \leftarrow \sigma_r \\
%     \text{End For } r \\
    \\
    \text{Result: } & A_k = U_k \Sigma_k V_k^T = \sum_{r=0}^{k-1} \sigma_r \mathbf{u}_r \mathbf{v}_r^T
\end{align*}
\subsubsection{Pseudocode Implementation}

\begin{lstlisting}[language=Python]
    let U be an m x k matrix (for left vectors)
    let V be an n x k matrix (for right vectors)

    for r from 0 to k-1:
        v = random normalised n-dimensional vector

        for i from 0 to some_limit:
            u = A * v

            // Orthogonalize u against previous U vectors (cleaning components in that direction)
            for j from 0 to r-1:
                u_prev = column j of U
                projection = dot(u, u_prev)
                u = u - projection * u_prev

            u = norm(u)

            w = A^T * u

            // again clean w
            for j from 0 to r-1:
                v_prev = column j of V
                projection = dot(w, v_prev)
                w = w - projection * v_prev

            sigma = norm(w)
            if sigma is near-zero:
                break // This component is negligible
            v = w / sigma

            // Check for convergence
            if dot(v, v_prev) is close to 1:
                break

        set column r of U = u
        set column r of V = v

    return U, V
\end{lstlisting}
\subsection{Code}
The implemenation is done in pure C. Image handling is done using stbimage.h. Other than that, only standard C libraries are used. The project also uses the standard OpenMP libary to implement intelligent multi-threaded processing of the loops, which highly speeds up the SVD process.
\newpage
\section{RESULTS}
\subsection{Example Outputs}
For each of the given input images, the following images were reconstructed:
\begin{figure}[H]
\centering
\includegraphics[width=0.2\columnwidth]{figs/einstein.jpg}
\includegraphics[width=0.2\columnwidth]{figs/einstein_5.jpg}
\includegraphics[width=0.2\columnwidth]{figs/einstein_10.jpg}
\includegraphics[width=0.2\columnwidth]{figs/einstein_25.jpg}
\includegraphics[width=0.2\columnwidth]{figs/einstein_50.jpg}
\includegraphics[width=0.2\columnwidth]{figs/einstein_100.jpg}
\includegraphics[width=0.2\columnwidth]{figs/einstein_200.jpg}
\caption{einstein.jpg: Original, followed by reconstructions for ranks 5, 10, 25, 50, 100 and 200}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.2\columnwidth]{figs/globe.jpg}
\includegraphics[width=0.2\columnwidth]{figs/globe_5.jpg}
\includegraphics[width=0.2\columnwidth]{figs/globe_10.jpg}
\includegraphics[width=0.2\columnwidth]{figs/globe_25.jpg}
\includegraphics[width=0.2\columnwidth]{figs/globe_50.jpg}
\includegraphics[width=0.2\columnwidth]{figs/globe_100.jpg}
\includegraphics[width=0.2\columnwidth]{figs/globe_200.jpg}
\caption{globe.jpg: Original, followed by reconstructions for ranks 5, 10, 25, 50, 100 and 200}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[width=0.2\columnwidth]{figs/greyscale.png}
\includegraphics[width=0.2\columnwidth]{figs/greyscale_5.png}
\includegraphics[width=0.2\columnwidth]{figs/greyscale_10.png}
\includegraphics[width=0.2\columnwidth]{figs/greyscale_25.png}
\includegraphics[width=0.2\columnwidth]{figs/greyscale_50.png}
\includegraphics[width=0.2\columnwidth]{figs/greyscale_100.png}
\includegraphics[width=0.2\columnwidth]{figs/greyscale_200.png}
\caption{greyscale.png: Original, followed by reconstructions for ranks 5, 10, 25, 50, 100 and 200}
\end{figure}
\subsection{Error Analysis}
Error analysis was performed by applying the Frobenius norm on the difference of the source and approximated matrices:
\begin{table}[h!]
 \centering
 \caption{Errors for einstein.jpg}
 \input{tables/einstein.tex}
\end{table}
\begin{table}[h!]
 \centering
 \caption{Errors for globe.jpg}
 \input{tables/globe.tex}
\end{table}
\begin{table}[h!]
 \centering
 \caption{Errors for greyscale.png}
 \input{tables/greyscale.tex}
\end{table}
\subsection{Reflections} \label{disc}

\end{document}
